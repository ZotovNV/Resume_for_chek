{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBx9OUPeJm6q1t9i9oOjkN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZotovNV/Resume_for_chek/blob/main/Python_%D0%9E%D1%81%D0%BD%D0%BE%D0%B2%D1%8B_%D0%B2%D0%B5%D0%B1_%D1%81%D0%BA%D1%80%D0%B0%D0%BF%D0%B8%D0%BD%D0%B3%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n28t59oDH-eE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Как показывали через 2 функции. Пока что по одной странице.\n",
        "\n",
        "def get_habr_pub(query):\n",
        "    habr_pub = pd.DataFrame()\n",
        "    for q in query:\n",
        "        url = 'https://habr.com/ru/search/'\n",
        "        a = url.split('/ru')\n",
        "        params = {\n",
        "            'q': q\n",
        "        }\n",
        "        req = requests.get(url, params=params)\n",
        "        time.sleep(0.3)\n",
        "        soup = BeautifulSoup(req.text)\n",
        "        articles = soup.find_all('article', class_='tm-articles-list__item')\n",
        "        for article in articles:\n",
        "            title = article.find('h2', class_='tm-title tm-title_h2').text\n",
        "            link = a[0] + (article.find('a', class_='tm-title__link').get('href'))\n",
        "            date = article.find('time').get('title')\n",
        "            votes = article.find('div', class_='tm-votes-meter tm-data-icons__item').text\n",
        "            row = {'date': date, 'title': title, 'link': link, 'votes': votes}\n",
        "            habr_pub = pd.concat([habr_pub, pd.DataFrame([row])])\n",
        "    return habr_pub.reset_index(drop=True)\n",
        "\n",
        "result = get_habr_pub(['python'])\n",
        "result"
      ],
      "metadata": {
        "id": "CY-Kdt8QIOED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_full_text(posts_df):\n",
        "    i = 0\n",
        "    for el in posts_df['link']:\n",
        "        # print(el)\n",
        "        req = requests.get(el).text\n",
        "        soup = BeautifulSoup(req)\n",
        "        time.sleep(0.3)\n",
        "        text = soup.find_all('div', class_='tm-article-body')\n",
        "        for ar in text:\n",
        "                full_text = ar.find('div', xmlns ='http://www.w3.org/1999/xhtml').text\n",
        "        posts_df.loc[i, 'text'] = full_text\n",
        "        i += 1\n",
        "    return posts_df\n",
        "\n",
        "add_full_text(result)"
      ],
      "metadata": {
        "id": "M08uZuym5YDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это второй вариант решения, с коментариями мелкими:"
      ],
      "metadata": {
        "id": "d42JbiZ46O9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# поступил своеобразно, т.к. каждый проход мы извлекаем ссылку, так почему бы сразу не вытаскивать и текст по этой ссылке. Осталось прикрутить страницы, но с ними сложность вижу что есть в ссылке такое\n",
        "# https://habr.com/ru/search/page1/?q=python&target_type=posts&order=relevance меня интересует page1, но как передавать в параметры так и не понял 'page': i ну не работает). Я так понимаю из за того что за место page\n",
        "#  ставлю число, а надо как то через конкатенацию что ли 'page':'page' + 'i'.\n",
        "# По логике по началу должен быть цикл на сам текст запроса, потом цикл по числу, ну и окончательное это цикл по каждой новости(в ней же завернут цикл по извлечению полного текста). В общем то не очень хорошо, то что цикл в цикле на цикле =D.\n",
        "# Но захотелось глянуть)\n",
        "# Если смотреть через первый вариант решения, то там сложновато все...Последним будет прогоняться функция которая полный текст извлекает. Не вижу варианта сделать в три функции. Хоть и вариант должен быть.\n",
        "\n",
        "def get_habr_pub(query):\n",
        "    habr_pub = pd.DataFrame()\n",
        "    for q in query:\n",
        "        url = 'https://habr.com/ru/search/'\n",
        "        a = url.split('/ru')\n",
        "        params = {\n",
        "            'q': q\n",
        "        }\n",
        "        req = requests.get(url, params=params)\n",
        "        time.sleep(0.3)\n",
        "        soup = BeautifulSoup(req.text)\n",
        "        articles = soup.find_all('article', class_='tm-articles-list__item')\n",
        "        for article in articles:\n",
        "            title = article.find('h2', class_='tm-title tm-title_h2').text\n",
        "            link = a[0] + (article.find('a', class_='tm-title__link').get('href'))\n",
        "            date = article.find('time').get('title')\n",
        "            votes = article.find('div', class_='tm-votes-meter tm-data-icons__item').text\n",
        "            text_pub = (BeautifulSoup(requests.get(link).text)).find_all('div', class_='tm-article-body')\n",
        "            for ar in text_pub:\n",
        "                text = ar.find('div', xmlns ='http://www.w3.org/1999/xhtml').text\n",
        "            row = {'date': date, 'title': title, 'link': link, 'votes': votes, 'text':text}\n",
        "            habr_pub = pd.concat([habr_pub, pd.DataFrame([row])])\n",
        "    return habr_pub.reset_index(drop=True)\n",
        "\n",
        "get_habr_pub(['python', 'Анализ данных'])"
      ],
      "metadata": {
        "id": "GgovlIz-SQF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# а это просто шаблончик того что происходит тут << text_pub = (BeautifulSoup(requests.get(link).text)).find_all('div', class_='tm-article-body') >>\n",
        "url_lib = 'https://habr.com/ru/news/531402/'\n",
        "\n",
        "rez = requests.get(url_lib)\n",
        "souz = BeautifulSoup(rez.text)\n",
        "art = souz.find_all('div', class_='tm-article-body')\n",
        "#print(art)\n",
        "for ar in art:\n",
        "    text = ar.find('div', xmlns ='http://www.w3.org/1999/xhtml').text\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "NK1CwGY1M7ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# проход по страницам, от 1 до 4. можно в функцию вставить input и указать там кол-во финальное страниц для сбора информации.\n",
        "def get_habr_pub(query):\n",
        "    habr_pub = pd.DataFrame()\n",
        "    for q in query:\n",
        "        url = 'https://habr.com/ru/search/'\n",
        "        a = url.split('/ru')\n",
        "        for page in range(1, 4):\n",
        "            params = {\n",
        "                'q': q,\n",
        "                'page': page\n",
        "            }\n",
        "            req = requests.get(url, params=params)\n",
        "            time.sleep(0.3)\n",
        "            soup = BeautifulSoup(req.text)\n",
        "            articles = soup.find_all('article', class_='tm-articles-list__item')\n",
        "            for article in articles:\n",
        "                title = article.find('h2', class_='tm-title tm-title_h2').text\n",
        "                link = a[0] + (article.find('a', class_='tm-title__link').get('href'))\n",
        "                date = article.find('time').get('title')\n",
        "                votes = article.find('div', class_='tm-votes-meter tm-data-icons__item').text\n",
        "                text_pub = (BeautifulSoup(requests.get(link).text)).find_all('div', class_='tm-article-body')\n",
        "                for ar in text_pub:\n",
        "                    text = ar.find('div', xmlns ='http://www.w3.org/1999/xhtml').text\n",
        "                row = {'date': date, 'title': title, 'link': link, 'votes': votes, 'text':text}\n",
        "                habr_pub = pd.concat([habr_pub, pd.DataFrame([row])])\n",
        "    return habr_pub.reset_index(drop=True)\n",
        "\n",
        "get_habr_pub(['python'])\n",
        "\n",
        "# В этом случае функция будет запрашивать результаты с первых трех страниц поиска и объединять их в один датафрейм. Если нужно получить результаты с большего количества страниц, можно изменить диапазон цикла \"for page in range(1, 4)\" на нужный."
      ],
      "metadata": {
        "id": "7Fx38Pf6dz-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}